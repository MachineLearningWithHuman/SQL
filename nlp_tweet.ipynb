{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Solutions"},{"metadata":{},"cell_type":"markdown","source":"What I have Answered In this NoteBook:\n1. Topic Clustering \n2. Topic labeling\n3. Topic Identification\n4. Sentiment Analysis\n5. Finding Insights from Sentiment analysis"},{"metadata":{},"cell_type":"markdown","source":"Sorry I can't write a whole paragraph for each of the set's I have mostly redirected some of the task to my old works as i am in hospital"},{"metadata":{},"cell_type":"markdown","source":"# topic clustering"},{"metadata":{},"cell_type":"markdown","source":"process: text processing : which follows lowering, removing most frequent and rear words ,removing emoji,emojicon etc and finally doing lemmenization upon them so we will get only tokens of words ;\nremember i have shown here tweets related to english only use appropiate lemme function to deal with as many as languages. then run a k mean clustering on the tokens to identify appropiate topics use t-sne for visualization and elbow curve for number of tokens.\nyour clustering done ."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/customer-support-on-twitter/sample.csv\n/kaggle/input/customer-support-on-twitter/twcs/twcs.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tweet =pd.read_csv(\"../input/customer-support-on-twitter/twcs/twcs.csv\")\ntweet_text = tweet[[\"text\"]]\ntweet_text[\"text\"] = tweet_text[\"text\"].astype(str)\ntweet.head()","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   tweet_id   author_id  inbound                      created_at  \\\n0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n\n                                                text response_tweet_id  \\\n0  @115712 I understand. I would like to assist y...                 2   \n1      @sprintcare and how do you propose we do that               NaN   \n2  @sprintcare I have sent several private messag...                 1   \n3  @115712 Please send us a Private Message so th...                 3   \n4                                 @sprintcare I did.                 4   \n\n   in_response_to_tweet_id  \n0                      3.0  \n1                      1.0  \n2                      4.0  \n3                      5.0  \n4                      6.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>author_id</th>\n      <th>inbound</th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>response_tweet_id</th>\n      <th>in_response_to_tweet_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>2</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>1</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>3</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n      <td>@sprintcare I did.</td>\n      <td>4</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_text.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                                text\n0  @115712 I understand. I would like to assist y...\n1      @sprintcare and how do you propose we do that\n2  @sprintcare I have sent several private messag...\n3  @115712 Please send us a Private Message so th...\n4                                 @sprintcare I did.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(2811774, 7)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.isna().sum()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"tweet_id                         0\nauthor_id                        0\ninbound                          0\ncreated_at                       0\ntext                             0\nresponse_tweet_id          1040629\nin_response_to_tweet_id     794335\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_text[\"text_lower\"]=tweet_text[\"text\"].str.lower()\ntweet_text","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                                                      text  \\\n0        @115712 I understand. I would like to assist y...   \n1            @sprintcare and how do you propose we do that   \n2        @sprintcare I have sent several private messag...   \n3        @115712 Please send us a Private Message so th...   \n4                                       @sprintcare I did.   \n...                                                    ...   \n2811769  @823869 Hey, we'd be happy to look into this f...   \n2811770  @115714 wtf!? I’ve been having really shitty s...   \n2811771  @143549 @sprintcare You have to go to https://...   \n2811772  @823870 Sounds delicious, Sarah! 😋 https://t.c...   \n2811773  @AldiUK  warm sloe gin mince pies with ice cre...   \n\n                                                text_lower  \n0        @115712 i understand. i would like to assist y...  \n1            @sprintcare and how do you propose we do that  \n2        @sprintcare i have sent several private messag...  \n3        @115712 please send us a private message so th...  \n4                                       @sprintcare i did.  \n...                                                    ...  \n2811769  @823869 hey, we'd be happy to look into this f...  \n2811770  @115714 wtf!? i’ve been having really shitty s...  \n2811771  @143549 @sprintcare you have to go to https://...  \n2811772  @823870 sounds delicious, sarah! 😋 https://t.c...  \n2811773  @aldiuk  warm sloe gin mince pies with ice cre...  \n\n[2811774 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2811769</th>\n      <td>@823869 Hey, we'd be happy to look into this f...</td>\n      <td>@823869 hey, we'd be happy to look into this f...</td>\n    </tr>\n    <tr>\n      <th>2811770</th>\n      <td>@115714 wtf!? I’ve been having really shitty s...</td>\n      <td>@115714 wtf!? i’ve been having really shitty s...</td>\n    </tr>\n    <tr>\n      <th>2811771</th>\n      <td>@143549 @sprintcare You have to go to https://...</td>\n      <td>@143549 @sprintcare you have to go to https://...</td>\n    </tr>\n    <tr>\n      <th>2811772</th>\n      <td>@823870 Sounds delicious, Sarah! 😋 https://t.c...</td>\n      <td>@823870 sounds delicious, sarah! 😋 https://t.c...</td>\n    </tr>\n    <tr>\n      <th>2811773</th>\n      <td>@AldiUK  warm sloe gin mince pies with ice cre...</td>\n      <td>@aldiuk  warm sloe gin mince pies with ice cre...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2811774 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.drop([\"text_lower\"], axis=1, inplace=True)\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ntweet_text[\"text_wo_punct\"] = tweet_text[\"text_lower\"].apply(lambda text: remove_punctuation(text))\ntweet_text.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \n0  115712 i understand i would like to assist you...  \n1       sprintcare and how do you propose we do that  \n2  sprintcare i have sent several private message...  \n3  115712 please send us a private message so tha...  \n4                                   sprintcare i did  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntweet_text[\"text_wo_stop\"] = tweet_text[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ntweet_text.head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \\\n0  115712 i understand i would like to assist you...   \n1       sprintcare and how do you propose we do that   \n2  sprintcare i have sent several private message...   \n3  115712 please send us a private message so tha...   \n4                                   sprintcare i did   \n\n                                        text_wo_stop  \n0  115712 understand would like assist would need...  \n1                                 sprintcare propose  \n2  sprintcare sent several private messages one r...  \n3  115712 please send us private message assist c...  \n4                                         sprintcare  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n      <td>115712 understand would like assist would need...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n      <td>sprintcare propose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n      <td>sprintcare sent several private messages one r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n      <td>115712 please send us private message assist c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n      <td>sprintcare</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncnt = Counter()\nfor text in tweet_text[\"text_wo_stop\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[('us', 451262),\n ('please', 402709),\n ('dm', 335374),\n ('help', 267633),\n ('hi', 224603),\n ('thanks', 206452),\n ('get', 200374),\n ('sorry', 192246),\n ('like', 146385),\n ('know', 145407)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"FREQWORDS = set([w for (w, wc) in cnt.most_common(15)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ntweet_text[\"text_wo_stopfreq\"] = tweet_text[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\ntweet_text.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \\\n0  115712 i understand i would like to assist you...   \n1       sprintcare and how do you propose we do that   \n2  sprintcare i have sent several private message...   \n3  115712 please send us a private message so tha...   \n4                                   sprintcare i did   \n\n                                        text_wo_stop  \\\n0  115712 understand would like assist would need...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 please send us private message assist c...   \n4                                         sprintcare   \n\n                                    text_wo_stopfreq  \n0  115712 understand would assist would need priv...  \n1                                 sprintcare propose  \n2  sprintcare sent several private messages one r...  \n3  115712 private message assist click ‘message’ ...  \n4                                         sprintcare  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_wo_stopfreq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n      <td>115712 understand would like assist would need...</td>\n      <td>115712 understand would assist would need priv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n      <td>sprintcare propose</td>\n      <td>sprintcare propose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>sprintcare sent several private messages one r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n      <td>115712 please send us private message assist c...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n      <td>sprintcare</td>\n      <td>sprintcare</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n\ntweet_text[\"text_wo_stopfreqrare\"] = tweet_text[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\ntweet_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\n# Drop the two columns \n#tweet_text.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ntweet_text[\"text_stemmed\"] = tweet_text[\"text_lower\"].apply(lambda text: stem_words(text))\ntweet_text.head()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \\\n0  115712 i understand i would like to assist you...   \n1       sprintcare and how do you propose we do that   \n2  sprintcare i have sent several private message...   \n3  115712 please send us a private message so tha...   \n4                                   sprintcare i did   \n\n                                        text_wo_stop  \\\n0  115712 understand would like assist would need...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 please send us private message assist c...   \n4                                         sprintcare   \n\n                                    text_wo_stopfreq  \\\n0  115712 understand would assist would need priv...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 private message assist click ‘message’ ...   \n4                                         sprintcare   \n\n                                        text_stemmed  \n0  @115712 i understand. i would like to assist y...  \n1        @sprintcar and how do you propos we do that  \n2  @sprintcar i have sent sever privat messag and...  \n3  @115712 pleas send us a privat messag so that ...  \n4                                  @sprintcar i did.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_wo_stopfreq</th>\n      <th>text_stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n      <td>115712 understand would like assist would need...</td>\n      <td>115712 understand would assist would need priv...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n      <td>sprintcare propose</td>\n      <td>sprintcare propose</td>\n      <td>@sprintcar and how do you propos we do that</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>@sprintcar i have sent sever privat messag and...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n      <td>115712 please send us private message assist c...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n      <td>@115712 pleas send us a privat messag so that ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n      <td>sprintcare</td>\n      <td>sprintcare</td>\n      <td>@sprintcar i did.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\ntweet_text[\"text_removalurl\"] = tweet_text[\"text_wo_stopfreq\"].apply(lambda text: remove_urls(text))\ntweet_text.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \\\n0  115712 i understand i would like to assist you...   \n1       sprintcare and how do you propose we do that   \n2  sprintcare i have sent several private message...   \n3  115712 please send us a private message so tha...   \n4                                   sprintcare i did   \n\n                                        text_wo_stop  \\\n0  115712 understand would like assist would need...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 please send us private message assist c...   \n4                                         sprintcare   \n\n                                    text_wo_stopfreq  \\\n0  115712 understand would assist would need priv...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 private message assist click ‘message’ ...   \n4                                         sprintcare   \n\n                                        text_stemmed  \\\n0  @115712 i understand. i would like to assist y...   \n1        @sprintcar and how do you propos we do that   \n2  @sprintcar i have sent sever privat messag and...   \n3  @115712 pleas send us a privat messag so that ...   \n4                                  @sprintcar i did.   \n\n                                     text_removalurl  \n0  115712 understand would assist would need priv...  \n1                                 sprintcare propose  \n2  sprintcare sent several private messages one r...  \n3  115712 private message assist click ‘message’ ...  \n4                                         sprintcare  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_wo_stopfreq</th>\n      <th>text_stemmed</th>\n      <th>text_removalurl</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n      <td>115712 understand would like assist would need...</td>\n      <td>115712 understand would assist would need priv...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 understand would assist would need priv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n      <td>sprintcare propose</td>\n      <td>sprintcare propose</td>\n      <td>@sprintcar and how do you propos we do that</td>\n      <td>sprintcare propose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>@sprintcar i have sent sever privat messag and...</td>\n      <td>sprintcare sent several private messages one r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n      <td>115712 please send us private message assist c...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n      <td>@115712 pleas send us a privat messag so that ...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n      <td>sprintcare</td>\n      <td>sprintcare</td>\n      <td>@sprintcar i did.</td>\n      <td>sprintcare</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"chat_words_str = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher\n\"\"\"","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\":\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)\n\ndef chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\n\nprint(chat_words_conversion(\"one minute BRB\"))\ntweet_text[\"text_chat_words_conversion\"] = tweet_text[\"text_removalurl\"].apply(lambda text: chat_words_conversion(text))\ntweet_text.head()","execution_count":18,"outputs":[{"output_type":"stream","text":"one minute Be Right Back\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"                                                text  \\\n0  @115712 I understand. I would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare I have sent several private messag...   \n3  @115712 Please send us a Private Message so th...   \n4                                 @sprintcare I did.   \n\n                                          text_lower  \\\n0  @115712 i understand. i would like to assist y...   \n1      @sprintcare and how do you propose we do that   \n2  @sprintcare i have sent several private messag...   \n3  @115712 please send us a private message so th...   \n4                                 @sprintcare i did.   \n\n                                       text_wo_punct  \\\n0  115712 i understand i would like to assist you...   \n1       sprintcare and how do you propose we do that   \n2  sprintcare i have sent several private message...   \n3  115712 please send us a private message so tha...   \n4                                   sprintcare i did   \n\n                                        text_wo_stop  \\\n0  115712 understand would like assist would need...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 please send us private message assist c...   \n4                                         sprintcare   \n\n                                    text_wo_stopfreq  \\\n0  115712 understand would assist would need priv...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 private message assist click ‘message’ ...   \n4                                         sprintcare   \n\n                                        text_stemmed  \\\n0  @115712 i understand. i would like to assist y...   \n1        @sprintcar and how do you propos we do that   \n2  @sprintcar i have sent sever privat messag and...   \n3  @115712 pleas send us a privat messag so that ...   \n4                                  @sprintcar i did.   \n\n                                     text_removalurl  \\\n0  115712 understand would assist would need priv...   \n1                                 sprintcare propose   \n2  sprintcare sent several private messages one r...   \n3  115712 private message assist click ‘message’ ...   \n4                                         sprintcare   \n\n                          text_chat_words_conversion  \n0  115712 understand would assist would need priv...  \n1                                 sprintcare propose  \n2  sprintcare sent several private messages one r...  \n3  115712 private message assist click ‘message’ ...  \n4                                         sprintcare  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_lower</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_wo_stopfreq</th>\n      <th>text_stemmed</th>\n      <th>text_removalurl</th>\n      <th>text_chat_words_conversion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@115712 I understand. I would like to assist y...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 i understand i would like to assist you...</td>\n      <td>115712 understand would like assist would need...</td>\n      <td>115712 understand would assist would need priv...</td>\n      <td>@115712 i understand. i would like to assist y...</td>\n      <td>115712 understand would assist would need priv...</td>\n      <td>115712 understand would assist would need priv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>@sprintcare and how do you propose we do that</td>\n      <td>sprintcare and how do you propose we do that</td>\n      <td>sprintcare propose</td>\n      <td>sprintcare propose</td>\n      <td>@sprintcar and how do you propos we do that</td>\n      <td>sprintcare propose</td>\n      <td>sprintcare propose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@sprintcare I have sent several private messag...</td>\n      <td>@sprintcare i have sent several private messag...</td>\n      <td>sprintcare i have sent several private message...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>@sprintcar i have sent sever privat messag and...</td>\n      <td>sprintcare sent several private messages one r...</td>\n      <td>sprintcare sent several private messages one r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@115712 Please send us a Private Message so th...</td>\n      <td>@115712 please send us a private message so th...</td>\n      <td>115712 please send us a private message so tha...</td>\n      <td>115712 please send us private message assist c...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n      <td>@115712 pleas send us a privat messag so that ...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n      <td>115712 private message assist click ‘message’ ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sprintcare I did.</td>\n      <td>@sprintcare i did.</td>\n      <td>sprintcare i did</td>\n      <td>sprintcare</td>\n      <td>sprintcare</td>\n      <td>@sprintcar i did.</td>\n      <td>sprintcare</td>\n      <td>sprintcare</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker","execution_count":19,"outputs":[{"output_type":"stream","text":"Collecting pyspellchecker\n  Downloading pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9 MB)\n\u001b[K     |████████████████████████████████| 1.9 MB 5.0 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.5.4\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install spacy_cld","execution_count":20,"outputs":[{"output_type":"stream","text":"Collecting spacy_cld\n  Downloading spacy_cld-0.1.0.tar.gz (3.3 kB)\nRequirement already satisfied: spacy<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy_cld) (2.2.4)\nCollecting pycld2>=0.31\n  Downloading pycld2-0.41.tar.gz (41.4 MB)\n\u001b[K     |████████████████████████████████| 41.4 MB 131 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (2.23.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.1.3)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.18.5)\nRequirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (7.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (46.1.3.post20200325)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (0.4.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (4.45.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (2.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (3.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (0.7.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (2020.6.20)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.0.0->spacy_cld) (1.7.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.0.0->spacy_cld) (3.1.0)\nBuilding wheels for collected packages: spacy-cld, pycld2\n  Building wheel for spacy-cld (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for spacy-cld: filename=spacy_cld-0.1.0-py3-none-any.whl size=4064 sha256=454d83fd99c748f83ab4156e41a9f3fe42af8eb88cff61cdfa204c85028fcec8\n  Stored in directory: /root/.cache/pip/wheels/82/6b/8f/260626615a2629391aca1fce5d3c676dde154f8aabb18d25bf\n  Building wheel for pycld2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834351 sha256=a186314c5b890401d06536950cda06bec4df23d0e9c4c21472c401d218218339\n  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\nSuccessfully built spacy-cld pycld2\nInstalling collected packages: pycld2, spacy-cld\nSuccessfully installed pycld2-0.41 spacy-cld-0.1.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ntweet_text[\"text_lemmatized\"] = tweet_text[\"text_chat_words_conversion\"].apply(lambda text: lemmatize_words(text))\ntweet_text.head()","execution_count":21,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-77c8f33811ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtweet_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_lemmatized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_chat_words_conversion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-21-77c8f33811ac>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtweet_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_lemmatized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_chat_words_conversion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-77c8f33811ac>\u001b[0m in \u001b[0;36mlemmatize_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwordnet_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"V\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"J\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"R\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADV\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpos_tagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tagged_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m_get_features\u001b[0;34m(self, i, word, context, prev, prev2)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i-1 word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i-1 suffix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i-2 word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i+1 word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i+1 suffix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         '''\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(lst): \n    return (lst[0].split())\ntweet_text[\"text_finilaized\"] = tweet_text[\"text_lemmatized\"].apply(lambda text: convert(text))\ntweet_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n  \nfrom nltk.cluster import KMeansClusterer\nimport nltk\nimport numpy as np \n  \nfrom sklearn import cluster\nfrom sklearn import metrics\n  \n# training data\n\"\"\" \nsentences = [['this', 'is', 'the', 'one','good', 'machine', 'learning', 'book'],\n            ['this', 'is',  'another', 'book'],\n            ['one', 'more', 'book'],\n            ['weather', 'rain', 'snow'],\n            ['yesterday', 'weather', 'snow'],\n            ['forecast', 'tomorrow', 'rain', 'snow'],\n            ['this', 'is', 'the', 'new', 'post'],\n            ['this', 'is', 'about', 'more', 'machine', 'learning', 'post'],  \n            ['and', 'this', 'is', 'the', 'one', 'last', 'post', 'book']]\n\"\"\"\nsentences = tweet_text[\"text_lemmatized\"]\n  \n \nmodel = Word2Vec(sentences, min_count=1)\n \n  \ndef sent_vectorizer(sent, model):\n    sent_vec =[]\n    numw = 0\n    for w in sent:\n        try:\n            if numw == 0:\n                sent_vec = model[w]\n            else:\n                sent_vec = np.add(sent_vec, model[w])\n            numw+=1\n        except:\n            pass\n     \n    return np.asarray(sent_vec) / numw\n  \n  \nX=[]\nfor sentence in sentences:\n    X.append(sent_vectorizer(sentence, model))   \n \nprint (\"========================\")\nprint (X)\n \n \n  \n \n# note with some version you would need use this (without wv) \n#  model[model.vocab] \nprint (model[model.wv.vocab])\n \n \n  \n \nprint (model.similarity('post', 'book'))\nprint (model.most_similar(positive=['machine'], negative=[], topn=2))\n\nNUM_CLUSTERS=2\nkclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\nassigned_clusters = kclusterer.cluster(X, assign_clusters=True)\nprint (assigned_clusters)\n  \n  \n  \nfor index, sentence in enumerate(sentences):    \n    print (str(assigned_clusters[index]) + \":\" + str(sentence))\n \n     \n     \n     \nkmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\nkmeans.fit(X)\n  \nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n  \nprint (\"Cluster id labels for inputted data\")\nprint (labels)\nprint (\"Centroids data\")\nprint (centroids)\n  \nprint (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\nprint (kmeans.score(X))\n  \nsilhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n  \nprint (\"Silhouette_score: \")\nprint (silhouette_score)\nimport matplotlib.pyplot as plt\n \nfrom sklearn.manifold import TSNE\n \nmodel = TSNE(n_components=2, random_state=0)\nnp.set_printoptions(suppress=True)\n \nY=model.fit_transform(X)\n \n \nplt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n \n \nfor j in range(len(sentences)):    \n   plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n   print (\"%s %s\" % (assigned_clusters[j],  sentences[j]))\n \n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# topic labeling"},{"metadata":{},"cell_type":"markdown","source":"once we have our labels we can train embedding to learn features in an embedding space for properly identify labels of our unseen data \ntreat it as classification (multiclass problem ) i am linking the demo solution here.\nhttps://github.com/MachineLearningWithHuman/python/blob/master/Advance_ML/NLP%20in%20tensorflow/embedding/bbc/BBC.ipynb\n"},{"metadata":{},"cell_type":"markdown","source":"some of the code with this dataset for labeling you can see here"},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding(tokenization)\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\nsentences=list(tweet_text[\"text\"])\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nprint(len(word_index))\n#print(word_index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding='post')\nprint(padded[0])\nprint(padded.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis"},{"metadata":{},"cell_type":"markdown","source":"we can build a sentiment analysis with explicit label as well as implicit labels \nexplicit model: we have to label the training data set use embedding and we can get good classifier i am linking imdb review work i had done here https://github.com/MachineLearningWithHuman/python/tree/master/Advance_ML/NLP%20in%20tensorflow/embedding/imdb\nimplicit model: we can get positive and negative label without explicit labelling from emoji's provided we have huge corpus of tweets of emoji's\nonce we find the sentiment we will get its topic from topic model \nNow to get the result of unhappyness let's say our topic model suggests a tweet is from amazoncare so we will subset all the tweet's based upon topics and we will run a topic clustering again to find the reasons or segregate graviances in different channel and run classifier again to find the reason.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n\nQnR = pd.merge(first_inbound, tweets, left_on='tweet_id', \n                                  right_on='in_response_to_tweet_id')\n\n# Filter to only outbound replies (from companies)\nQnR = QnR[QnR.inbound_y ^ True]\nprint(f'Data shape: {QnR.shape}')\nQnR.head()","execution_count":29,"outputs":[{"output_type":"stream","text":"Data shape: (794299, 14)\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"   tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n0           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n1           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n2           8      115712       True  Tue Oct 31 21:45:10 +0000 2017   \n3          18      115713       True  Tue Oct 31 19:56:01 +0000 2017   \n4          20      115715       True  Tue Oct 31 22:03:34 +0000 2017   \n\n                                              text_x response_tweet_id_x  \\\n0          @sprintcare is the worst customer service              9,6,10   \n1          @sprintcare is the worst customer service              9,6,10   \n2          @sprintcare is the worst customer service              9,6,10   \n3  @115714 y’all lie about your “great” connectio...                  17   \n4  @115714 whenever I contact customer support, t...                  19   \n\n   in_response_to_tweet_id_x  tweet_id_y author_id_y  inbound_y  \\\n0                        NaN           6  sprintcare      False   \n1                        NaN           9  sprintcare      False   \n2                        NaN          10  sprintcare      False   \n3                        NaN          17  sprintcare      False   \n4                        NaN          19  sprintcare      False   \n\n                     created_at_y  \\\n0  Tue Oct 31 21:46:24 +0000 2017   \n1  Tue Oct 31 21:46:14 +0000 2017   \n2  Tue Oct 31 21:45:59 +0000 2017   \n3  Tue Oct 31 19:59:13 +0000 2017   \n4  Tue Oct 31 22:10:10 +0000 2017   \n\n                                              text_y response_tweet_id_y  \\\n0  @115712 Can you please send us a private messa...                 5,7   \n1  @115712 I would love the chance to review the ...                 NaN   \n2  @115712 Hello! We never like our customers to ...                 NaN   \n3  @115713 H there! We'd definitely like to work ...                  16   \n4  @115715 Please send me a private message so th...                 NaN   \n\n   in_response_to_tweet_id_y  \n0                        8.0  \n1                        8.0  \n2                        8.0  \n3                       18.0  \n4                       20.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id_x</th>\n      <th>author_id_x</th>\n      <th>inbound_x</th>\n      <th>created_at_x</th>\n      <th>text_x</th>\n      <th>response_tweet_id_x</th>\n      <th>in_response_to_tweet_id_x</th>\n      <th>tweet_id_y</th>\n      <th>author_id_y</th>\n      <th>inbound_y</th>\n      <th>created_at_y</th>\n      <th>text_y</th>\n      <th>response_tweet_id_y</th>\n      <th>in_response_to_tweet_id_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>9,6,10</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 21:46:24 +0000 2017</td>\n      <td>@115712 Can you please send us a private messa...</td>\n      <td>5,7</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>9,6,10</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 21:46:14 +0000 2017</td>\n      <td>@115712 I would love the chance to review the ...</td>\n      <td>NaN</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>115712</td>\n      <td>True</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>9,6,10</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 21:45:59 +0000 2017</td>\n      <td>@115712 Hello! We never like our customers to ...</td>\n      <td>NaN</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>115713</td>\n      <td>True</td>\n      <td>Tue Oct 31 19:56:01 +0000 2017</td>\n      <td>@115714 y’all lie about your “great” connectio...</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>17</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 19:59:13 +0000 2017</td>\n      <td>@115713 H there! We'd definitely like to work ...</td>\n      <td>16</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n      <td>115715</td>\n      <td>True</td>\n      <td>Tue Oct 31 22:03:34 +0000 2017</td>\n      <td>@115714 whenever I contact customer support, t...</td>\n      <td>19</td>\n      <td>NaN</td>\n      <td>19</td>\n      <td>sprintcare</td>\n      <td>False</td>\n      <td>Tue Oct 31 22:10:10 +0000 2017</td>\n      <td>@115715 Please send me a private message so th...</td>\n      <td>NaN</td>\n      <td>20.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making sure the dataframe contains only the needed columns\nQnR = QnR[[\"author_id_x\",\"created_at_x\",\"text_x\",\"author_id_y\",\"created_at_y\",\"text_y\"]]\nQnR.head(5)","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"  author_id_x                    created_at_x  \\\n0      115712  Tue Oct 31 21:45:10 +0000 2017   \n1      115712  Tue Oct 31 21:45:10 +0000 2017   \n2      115712  Tue Oct 31 21:45:10 +0000 2017   \n3      115713  Tue Oct 31 19:56:01 +0000 2017   \n4      115715  Tue Oct 31 22:03:34 +0000 2017   \n\n                                              text_x author_id_y  \\\n0          @sprintcare is the worst customer service  sprintcare   \n1          @sprintcare is the worst customer service  sprintcare   \n2          @sprintcare is the worst customer service  sprintcare   \n3  @115714 y’all lie about your “great” connectio...  sprintcare   \n4  @115714 whenever I contact customer support, t...  sprintcare   \n\n                     created_at_y  \\\n0  Tue Oct 31 21:46:24 +0000 2017   \n1  Tue Oct 31 21:46:14 +0000 2017   \n2  Tue Oct 31 21:45:59 +0000 2017   \n3  Tue Oct 31 19:59:13 +0000 2017   \n4  Tue Oct 31 22:10:10 +0000 2017   \n\n                                              text_y  \n0  @115712 Can you please send us a private messa...  \n1  @115712 I would love the chance to review the ...  \n2  @115712 Hello! We never like our customers to ...  \n3  @115713 H there! We'd definitely like to work ...  \n4  @115715 Please send me a private message so th...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_id_x</th>\n      <th>created_at_x</th>\n      <th>text_x</th>\n      <th>author_id_y</th>\n      <th>created_at_y</th>\n      <th>text_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115712</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>sprintcare</td>\n      <td>Tue Oct 31 21:46:24 +0000 2017</td>\n      <td>@115712 Can you please send us a private messa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115712</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>sprintcare</td>\n      <td>Tue Oct 31 21:46:14 +0000 2017</td>\n      <td>@115712 I would love the chance to review the ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115712</td>\n      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n      <td>@sprintcare is the worst customer service</td>\n      <td>sprintcare</td>\n      <td>Tue Oct 31 21:45:59 +0000 2017</td>\n      <td>@115712 Hello! We never like our customers to ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115713</td>\n      <td>Tue Oct 31 19:56:01 +0000 2017</td>\n      <td>@115714 y’all lie about your “great” connectio...</td>\n      <td>sprintcare</td>\n      <td>Tue Oct 31 19:59:13 +0000 2017</td>\n      <td>@115713 H there! We'd definitely like to work ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115715</td>\n      <td>Tue Oct 31 22:03:34 +0000 2017</td>\n      <td>@115714 whenever I contact customer support, t...</td>\n      <td>sprintcare</td>\n      <td>Tue Oct 31 22:10:10 +0000 2017</td>\n      <td>@115715 Please send me a private message so th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nwidth = 0.75\ncount = QnR.groupby(\"author_id_y\")[\"text_x\"].count()\nc = count[count>15000].plot(kind='barh',figsize=(10, 8), color='#619CFF', zorder=2, width=width,)\nc.set_ylabel('')\nplt.show()","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"AS you can see here we are grouping the topics based upon author id's so we can defintly find topics/company/concern_party for each tweet our next task will be segregate those and apply same method as above to get underline reasons "},{"metadata":{},"cell_type":"markdown","source":"# Automated Reply\nwe can use encoder-decoder model to give reply to the concern user ... we can get motivation of using federated learning as google used in there smart reply feature on youtube"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport random\nimport time\nimport keras\nimport pandas as pd\nimport sklearn\nimport nltk\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import casual_tokenize\nfrom tqdm import tqdm_notebook as tqdm","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#8192 - large enough for demonstration, larger values make network training slower\nMAX_VOCAB_SIZE = 2**13\n# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n# but result in slower training and larger networks\nMAX_MESSAGE_LEN = 30  \n# Embedding size for words - gives a trade off between expressivity of words and network size\nEMBEDDING_SIZE = 100\n# Embedding size for whole messages, same trade off as word embeddings\nCONTEXT_SIZE = 100\n# Larger batch sizes generally reach the average response faster, but small batch sizes are\n# required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\nBATCH_SIZE = 4\n# Helps regularize network and prevent overfitting.\nDROPOUT = 0.2\n# High learning rate helps model reach average response faster, but can make it hard to \n# converge on nuanced responses\nLEARNING_RATE=0.005\n# Tokens needed for seq2seq\nUNK = 0  # words that aren't found in the vocab\nPAD = 1  # after message has finished, this fills all remaining vector positions\nSTART = 2  # provided to the model at position 0 for every response predicted\n\n# Implementaiton detail for allowing this to be run in Kaggle's notebook hardware\nSUB_BATCH_SIZE = 1000","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntweets = pd.read_csv('../input/customer-support-on-twitter/twcs/twcs.csv')\n\nfirst_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n\ninbounds_and_outbounds = pd.merge(first_inbound, tweets, left_on='tweet_id', \n                                  right_on='in_response_to_tweet_id').sample(frac=1)\n\n# Filter to only outbound replies (from companies)\ninbounds_and_outbounds = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n\ntqdm().pandas()","execution_count":25,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  # This is added back by InteractiveShellApp.init_path()\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d92998363046dcae462f04537a6a5a"}},"metadata":{}},{"output_type":"stream","text":"CPU times: user 15.3 s, sys: 3.82 s, total: 19.2 s\nWall time: 19.2 s\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"inbounds_and_outbounds.head()","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"        tweet_id_x author_id_x  inbound_x                    created_at_x  \\\n613649     2133969      627858       True  Sun Nov 26 13:25:16 +0000 2017   \n705238     2430160      697639       True  Tue Nov 14 19:22:58 +0000 2017   \n244368      902439      334240       True  Fri Oct 13 17:05:46 +0000 2017   \n354776     1291378      421996       True  Thu Oct 26 20:35:45 +0000 2017   \n313925     1146803      389919       True  Sun Oct 15 16:39:54 +0000 2017   \n\n                                                   text_x  \\\n613649  @Delta really 1 1/2 hours before my flight, tr...   \n705238  Fabulous fresh fruit from @Morrisons ready for...   \n244368  @AmazonHelp Curious: Why does (US) \"Alexa deal...   \n354776  Ok @SouthwestAir , I can’t hide it anymore. Pl...   \n313925  @Delta .. I'm on flight DL0801 tomorrow and yo...   \n\n            response_tweet_id_x  in_response_to_tweet_id_x  tweet_id_y  \\\n613649  2133967,2133971,2133972                        NaN     2133967   \n705238                  2430159                        NaN     2430159   \n244368                   902437                        NaN      902437   \n354776          1291376,1291380                        NaN     1291376   \n313925          1146802,1146804                        NaN     1146804   \n\n         author_id_y  inbound_y                    created_at_y  \\\n613649         Delta      False  Sun Nov 26 13:26:12 +0000 2017   \n705238     Morrisons      False  Tue Nov 14 20:40:33 +0000 2017   \n244368    AmazonHelp      False  Fri Oct 13 19:06:00 +0000 2017   \n354776  SouthwestAir      False  Thu Oct 26 20:43:46 +0000 2017   \n313925         Delta      False  Sun Oct 15 16:58:45 +0000 2017   \n\n                                                   text_y response_tweet_id_y  \\\n613649  @627858 Hi Robin. I can check to see if your f...             2133968   \n705238                            @697639 Enjoy!! - Chloe                 NaN   \n244368  @334240 We'd like to look into this with you d...              902438   \n354776  @421996 *Cue \"Treat You Better\" by Shawn Mende...             1291377   \n313925  @389919 Thank you for your patience.  Your ori...             1821964   \n\n        in_response_to_tweet_id_y  \n613649                  2133969.0  \n705238                  2430160.0  \n244368                   902439.0  \n354776                  1291378.0  \n313925                  1146803.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id_x</th>\n      <th>author_id_x</th>\n      <th>inbound_x</th>\n      <th>created_at_x</th>\n      <th>text_x</th>\n      <th>response_tweet_id_x</th>\n      <th>in_response_to_tweet_id_x</th>\n      <th>tweet_id_y</th>\n      <th>author_id_y</th>\n      <th>inbound_y</th>\n      <th>created_at_y</th>\n      <th>text_y</th>\n      <th>response_tweet_id_y</th>\n      <th>in_response_to_tweet_id_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>613649</th>\n      <td>2133969</td>\n      <td>627858</td>\n      <td>True</td>\n      <td>Sun Nov 26 13:25:16 +0000 2017</td>\n      <td>@Delta really 1 1/2 hours before my flight, tr...</td>\n      <td>2133967,2133971,2133972</td>\n      <td>NaN</td>\n      <td>2133967</td>\n      <td>Delta</td>\n      <td>False</td>\n      <td>Sun Nov 26 13:26:12 +0000 2017</td>\n      <td>@627858 Hi Robin. I can check to see if your f...</td>\n      <td>2133968</td>\n      <td>2133969.0</td>\n    </tr>\n    <tr>\n      <th>705238</th>\n      <td>2430160</td>\n      <td>697639</td>\n      <td>True</td>\n      <td>Tue Nov 14 19:22:58 +0000 2017</td>\n      <td>Fabulous fresh fruit from @Morrisons ready for...</td>\n      <td>2430159</td>\n      <td>NaN</td>\n      <td>2430159</td>\n      <td>Morrisons</td>\n      <td>False</td>\n      <td>Tue Nov 14 20:40:33 +0000 2017</td>\n      <td>@697639 Enjoy!! - Chloe</td>\n      <td>NaN</td>\n      <td>2430160.0</td>\n    </tr>\n    <tr>\n      <th>244368</th>\n      <td>902439</td>\n      <td>334240</td>\n      <td>True</td>\n      <td>Fri Oct 13 17:05:46 +0000 2017</td>\n      <td>@AmazonHelp Curious: Why does (US) \"Alexa deal...</td>\n      <td>902437</td>\n      <td>NaN</td>\n      <td>902437</td>\n      <td>AmazonHelp</td>\n      <td>False</td>\n      <td>Fri Oct 13 19:06:00 +0000 2017</td>\n      <td>@334240 We'd like to look into this with you d...</td>\n      <td>902438</td>\n      <td>902439.0</td>\n    </tr>\n    <tr>\n      <th>354776</th>\n      <td>1291378</td>\n      <td>421996</td>\n      <td>True</td>\n      <td>Thu Oct 26 20:35:45 +0000 2017</td>\n      <td>Ok @SouthwestAir , I can’t hide it anymore. Pl...</td>\n      <td>1291376,1291380</td>\n      <td>NaN</td>\n      <td>1291376</td>\n      <td>SouthwestAir</td>\n      <td>False</td>\n      <td>Thu Oct 26 20:43:46 +0000 2017</td>\n      <td>@421996 *Cue \"Treat You Better\" by Shawn Mende...</td>\n      <td>1291377</td>\n      <td>1291378.0</td>\n    </tr>\n    <tr>\n      <th>313925</th>\n      <td>1146803</td>\n      <td>389919</td>\n      <td>True</td>\n      <td>Sun Oct 15 16:39:54 +0000 2017</td>\n      <td>@Delta .. I'm on flight DL0801 tomorrow and yo...</td>\n      <td>1146802,1146804</td>\n      <td>NaN</td>\n      <td>1146804</td>\n      <td>Delta</td>\n      <td>False</td>\n      <td>Sun Oct 15 16:58:45 +0000 2017</td>\n      <td>@389919 Thank you for your patience.  Your ori...</td>\n      <td>1821964</td>\n      <td>1146803.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sn_replace(match):\n    _sn = match.group(2).lower()\n    if not _sn.isnumeric():\n        # This is a company screen name\n        return match.group(1) + match.group(2)\n    return '@__sn__'\n\nsn_re = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\nprint(\"Replacing anonymized screen names in X...\")\nx_text = inbounds_and_outbounds.text_x.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))\nprint(\"Replacing anonymized screen names in Y...\")\ny_text = inbounds_and_outbounds.text_y.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))","execution_count":27,"outputs":[{"output_type":"stream","text":"Replacing anonymized screen names in X...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=794299.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361f9ebd97584cc3a59a232c50822688"}},"metadata":{}},{"output_type":"stream","text":"\nReplacing anonymized screen names in Y...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=794299.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07b18a2ca45c41c698de123c18519498"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vec = CountVectorizer(tokenizer=casual_tokenize, max_features=MAX_VOCAB_SIZE - 3)\nprint(\"Fitting CountVectorizer on X and Y text data...\")\ncount_vec.fit(tqdm(x_text + y_text))\nanalyzer = count_vec.build_analyzer()\nvocab = {k: v + 3 for k, v in count_vec.vocabulary_.items()}\nvocab['__unk__'] = UNK\nvocab['__pad__'] = PAD\nvocab['__start__'] = START\n# Used to turn seq2seq predictions into human readable strings\nreverse_vocab = {v: k for k, v in vocab.items()}\nprint(f\"Learned vocab of {len(vocab)} items.\")","execution_count":28,"outputs":[{"output_type":"stream","text":"Fitting CountVectorizer on X and Y text data...\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=794299.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334f3c7b37a142b9b7cf46a84d1e3d95"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-b2421b64830e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasual_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_VOCAB_SIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting CountVectorizer on X and Y text data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \"\"\"\n\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1199\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36mcasual_tokenize\u001b[0;34m(text, preserve_case, reduce_len, strip_handles)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \"\"\"\n\u001b[1;32m    340\u001b[0m     return TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len,\n\u001b[0;32m--> 341\u001b[0;31m                           strip_handles=strip_handles).tokenize(text)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0msafe_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHANG_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\1\\1\\1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWORD_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Possibly alter the case, but avoid changing emoticons like :D into :d:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_case\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_word_idx(sentence):\n    full_length = [vocab.get(tok, UNK) for tok in analyzer(sentence)] + [PAD] * MAX_MESSAGE_LEN\n    return full_length[:MAX_MESSAGE_LEN]\n\ndef from_word_idx(word_idxs):\n    return ' '.join(reverse_vocab[idx] for idx in word_idxs if idx != PAD).strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Calculating word indexes for X...\")\nx = pd.np.vstack(x_text.progress_apply(to_word_idx).values)\nprint(\"Calculating word indexes for Y...\")\ny = pd.np.vstack(y_text.progress_apply(to_word_idx).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_idx = list(range(len(x)))\ntrain_idx = set(random.sample(all_idx, int(0.8 * len(all_idx))))\ntest_idx = {idx for idx in all_idx if idx not in train_idx}\n\ntrain_x = x[list(train_idx)]\ntest_x = x[list(test_idx)]\ntrain_y = y[list(train_idx)]\ntest_y = y[list(test_idx)]\n\nassert train_x.shape == train_y.shape\nassert test_x.shape == test_y.shape\n\nprint(f'Training data of shape {train_x.shape} and test data of shape {test_x.shape}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras imports, because there are like... A million of them.\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Input, LSTM, Dropout, Embedding, RepeatVector, concatenate, \\\n    TimeDistributed\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    shared_embedding = Embedding(\n        output_dim=EMBEDDING_SIZE,\n        input_dim=MAX_VOCAB_SIZE,\n        input_length=MAX_MESSAGE_LEN,\n        name='embedding',\n    )\n    \n    # ENCODER\n    \n    encoder_input = Input(\n        shape=(MAX_MESSAGE_LEN,),\n        dtype='int32',\n        name='encoder_input',\n    )\n    \n    embedded_input = shared_embedding(encoder_input)\n    \n    # No return_sequences - since the encoder here only produces a single value for the\n    # input sequence provided.\n    encoder_rnn = LSTM(\n        CONTEXT_SIZE,\n        name='encoder',\n        dropout=DROPOUT\n    )\n    \n    context = RepeatVector(MAX_MESSAGE_LEN)(encoder_rnn(embedded_input))\n    \n    # DECODER\n    \n    last_word_input = Input(\n        shape=(MAX_MESSAGE_LEN, ),\n        dtype='int32',\n        name='last_word_input',\n    )\n    \n    embedded_last_word = shared_embedding(last_word_input)\n    # Combines the context produced by the encoder and the last word uttered as inputs\n    # to the decoder.\n    decoder_input = concatenate([embedded_last_word, context], axis=2)\n    \n    # return_sequences causes LSTM to produce one output per timestep instead of one at the\n    # end of the intput, which is important for sequence producing models.\n    decoder_rnn = LSTM(\n        CONTEXT_SIZE,\n        name='decoder',\n        return_sequences=True,\n        dropout=DROPOUT)\n    decoder_output = decoder_rnn(decoder_input)\n    \n    # TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n    next_word_dense = TimeDistributed(\n        Dense(int(MAX_VOCAB_SIZE / 2), activation='relu'),\n        name='next_word_dense',\n    )(decoder_output)\n    \n    next_word = TimeDistributed(\n        Dense(MAX_VOCAB_SIZE, activation='softmax'),\n        name='next_word_softmax'\n    )(next_word_dense)\n    \n    return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n\ns2s_model = create_model()\noptimizer = Adam(lr=LEARNING_RATE, clipvalue=5.0)\ns2s_model.compile(optimizer='adam', loss='categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_start_token(y_array):\n    \"\"\" Adds the start token to vectors.  Used for training data. \"\"\"\n    return np.hstack([\n        START * np.ones((len(y_array), 1)),\n        y_array[:, :-1],\n    ])\n\ndef binarize_labels(labels):\n    \"\"\" Helper function that turns integer word indexes into sparse binary matrices for \n        the expected model output.\n    \"\"\"\n    return np.array([np_utils.to_categorical(row, num_classes=MAX_VOCAB_SIZE)\n                     for row in labels])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def respond_to(model, text):\n    \"\"\" Helper function that takes a text input and provides a text output. \"\"\"\n    input_y = add_start_token(PAD * np.ones((1, MAX_MESSAGE_LEN)))\n    idxs = np.array(to_word_idx(text)).reshape((1, MAX_MESSAGE_LEN))\n    for position in range(MAX_MESSAGE_LEN - 1):\n        prediction = model.predict([idxs, input_y]).argmax(axis=2)[0]\n        input_y[:,position + 1] = prediction[position]\n    return from_word_idx(model.predict([idxs, input_y]).argmax(axis=2)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_mini_epoch(model, start_idx, end_idx):\n    \"\"\" Batching seems necessary in Kaggle Jupyter Notebook environments, since\n        `model.fit` seems to freeze on larger batches (somewhere 1k-10k).\n    \"\"\"\n    b_train_y = binarize_labels(train_y[start_idx:end_idx])\n    input_train_y = add_start_token(train_y[start_idx:end_idx])\n    \n    model.fit(\n        [train_x[start_idx:end_idx], input_train_y], \n        b_train_y,\n        epochs=1,\n        batch_size=BATCH_SIZE,\n    )\n    \n    rand_idx = random.sample(list(range(len(test_x))), SUB_BATCH_SIZE)\n    print('Test results:', model.evaluate(\n        [test_x[rand_idx], add_start_token(test_y[rand_idx])],\n        binarize_labels(test_y[rand_idx])\n    ))\n    \n    input_strings = [\n        \"@AppleSupport I fix I this I stupid I problem I\",\n        \"@AmazonHelp I hadnt expected that such a big brand like amazon would have such a poor customer service.\",\n    ]\n     for input_string in input_strings:\n        output_string = respond_to(model, input_string)\n        print(f'> \"{input_string}\"\\n< \"{output_string}\"')\n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_time_limit = 360 * 60  # seconds (notebooks terminate after 1 hour)\nstart_time = time.time()\nstop_after = start_time + training_time_limit\n\nclass TimesUpInterrupt(Exception):\n    pass\n\ntry:\n    for epoch in range(100):\n        print(f'Training in epoch {epoch}...')\n        for start_idx in range(0, len(train_x), SUB_BATCH_SIZE):\n            train_mini_epoch(s2s_model, start_idx, start_idx + SUB_BATCH_SIZE)\n            if time.time() > stop_after:\n                raise TimesUpInterrupt\nexcept KeyboardInterrupt:\n    print(\"Halting training from keyboard interrupt.\")\nexcept TimesUpInterrupt:\n    print(f\"Halting after {time.time() - start_time} seconds spent training.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"respond_to(s2s_model, '''@AppleSupport iPhone 8 touchID doesnt unlock while charging on \n    110v w/ 61w laptop charger to usbc lightning cable just uh.. so you guys know''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorry I can't provide full codes for all as i am doing it from hospital."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}