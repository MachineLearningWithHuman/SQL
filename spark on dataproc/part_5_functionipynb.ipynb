{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part-5 functionipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "utrBdiyRca_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
        "gunzip kddcup.data_10_percent.gz\n",
        "BUCKET='cloud-training-demos-ml'  # CHANGE\n",
        "gsutil cp kdd* gs://$BUCKET/\n",
        "bq mk sparktobq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imDaiE2qceEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import google.cloud.storage as gcs\n",
        "import tempfile\n",
        "import os\n",
        "def create_report(BUCKET, gcsfilename, tmpdir):\n",
        "    \"\"\"\n",
        "    Creates report in gs://BUCKET/ based on contents in gcsfilename (gs://bucket/some/dir/filename)\n",
        "    \"\"\"\n",
        "    # connect to BigQuery\n",
        "    client = bigquery.Client()\n",
        "    destination_table = client.get_table('sparktobq.kdd_cup')\n",
        "    \n",
        "    # Specify table schema. Autodetect is not a good idea for production code\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"duration\", \"INT64\"),\n",
        "    ]\n",
        "    for name in ['protocol_type', 'service', 'flag']:\n",
        "        schema.append(bigquery.SchemaField(name, \"STRING\"))\n",
        "    for name in 'src_bytes,dst_bytes,wrong_fragment,urgent,hot,num_failed_logins'.split(','):\n",
        "        schema.append(bigquery.SchemaField(name, \"INT64\"))\n",
        "    schema.append(bigquery.SchemaField(\"unused_10\", \"STRING\"))\n",
        "    schema.append(bigquery.SchemaField(\"num_compromised\", \"INT64\"))\n",
        "    schema.append(bigquery.SchemaField(\"unused_12\", \"STRING\"))\n",
        "    for name in 'su_attempted,num_root,num_file_creations'.split(','):\n",
        "        schema.append(bigquery.SchemaField(name, \"INT64\")) \n",
        "    for fieldno in range(16, 41):\n",
        "        schema.append(bigquery.SchemaField(\"unused_{}\".format(fieldno), \"STRING\"))\n",
        "    schema.append(bigquery.SchemaField(\"label\", \"STRING\"))\n",
        "    job_config.schema = schema\n",
        "\n",
        "    # Load CSV data into BigQuery, replacing any rows that were there before\n",
        "    job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    job_config.skip_leading_rows = 0\n",
        "    job_config.source_format = bigquery.SourceFormat.CSV\n",
        "    load_job = client.load_table_from_uri(gcsfilename, destination_table, job_config=job_config)\n",
        "    print(\"Starting LOAD job {} for {}\".format(load_job.job_id, gcsfilename))\n",
        "    load_job.result()  # Waits for table load to complete.\n",
        "    print(\"Finished LOAD job {}\".format(load_job.job_id))\n",
        "    \n",
        "    # connections by protocol\n",
        "    sql = \"\"\"\n",
        "        SELECT COUNT(*) AS count\n",
        "        FROM sparktobq.kdd_cup\n",
        "        GROUP BY protocol_type\n",
        "        ORDER by count ASC    \n",
        "    \"\"\"\n",
        "    connections_by_protocol = client.query(sql).to_dataframe()\n",
        "    connections_by_protocol.to_csv(os.path.join(tmpdir,\"connections_by_protocol.csv\"))\n",
        "    print(\"Finished analyzing connections\")\n",
        "    \n",
        "    # attacks plot\n",
        "    sql = \"\"\"\n",
        "                            SELECT \n",
        "                             protocol_type, \n",
        "                             CASE label\n",
        "                               WHEN 'normal.' THEN 'no attack'\n",
        "                               ELSE 'attack'\n",
        "                             END AS state,\n",
        "                             COUNT(*) as total_freq,\n",
        "                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
        "                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
        "                             ROUND(AVG(duration), 2) as mean_duration,\n",
        "                             SUM(num_failed_logins) as total_failed_logins,\n",
        "                             SUM(num_compromised) as total_compromised,\n",
        "                             SUM(num_file_creations) as total_file_creations,\n",
        "                             SUM(su_attempted) as total_root_attempts,\n",
        "                             SUM(num_root) as total_root_acceses\n",
        "                           FROM sparktobq.kdd_cup\n",
        "                           GROUP BY protocol_type, state\n",
        "                           ORDER BY 3 DESC\n",
        "    \"\"\"\n",
        "    attack_stats = client.query(sql).to_dataframe()\n",
        "    ax = attack_stats.plot.bar(x='protocol_type', subplots=True, figsize=(10,25))\n",
        "    ax[0].get_figure().savefig(os.path.join(tmpdir,'report.png'));\n",
        "    print(\"Finished analyzing attacks\")\n",
        "    \n",
        "    bucket = gcs.Client().get_bucket(BUCKET)\n",
        "    for blob in bucket.list_blobs(prefix='sparktobq/'):\n",
        "        blob.delete()\n",
        "    for fname in ['report.png', 'connections_by_protocol.csv']:\n",
        "        bucket.blob('sparktobq/{}'.format(fname)).upload_from_filename(os.path.join(tmpdir,fname))\n",
        "    print(\"Uploaded report based on {} to {}\".format(gcsfilename, BUCKET))\n",
        "\n",
        "\n",
        "def bigquery_analysis_cf(data, context):\n",
        "    # check that trigger is for a file of interest\n",
        "    bucket = data['bucket']\n",
        "    name = data['name']\n",
        "    if ('kddcup' in name) and not ('gz' in name):\n",
        "        filename = 'gs://{}/{}'.format(bucket, data['name'])\n",
        "        print(bucket, filename)\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            create_report(bucket, filename, tmpdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCJDE782c1MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile requirements.txt\n",
        "google-cloud-bigquery\n",
        "google-cloud-storage\n",
        "pandas\n",
        "matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-KdBHFXc5Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# verify that the code in the CF works\n",
        "name='kddcup.data_10_percent'\n",
        "if 'kddcup' in name and not ('gz' in name):\n",
        "    print(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrFfvruic8qr",
        "colab_type": "text"
      },
      "source": [
        "# Test that the function endpoint works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZXd3Xzc72U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test that the function works\n",
        "import main as bq\n",
        "\n",
        "BUCKET='cloud-training-demos-ml' # CHANGE\n",
        "try:\n",
        "    bq.create_report(BUCKET, 'gs://{}/kddcup.data_10_percent'.format(BUCKET), \"/tmp\")\n",
        "except Exception as e:\n",
        "    print(e.errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUgJUCXdCIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud functions deploy bigquery_analysis_cf --runtime python37 --trigger-resource $BUCKET --trigger-event google.storage.object.finalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBbsenLdJVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil rm -rf gs://$BUCKET/sparktobq\n",
        "!gsutil cp kddcup.data_10_percent gs://$BUCKET/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYYYoaFVdLgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil ls gs://$BUCKET/sparktobq"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}