{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkonCloud1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb4j_l-kU0TB",
        "colab_type": "text"
      },
      "source": [
        "# Migrating from Spark to BigQuery via Dataproc(1)\n",
        "### Copy data to HDFS\n",
        "The data itself comes from the 1999 KDD competition. Let's grab 10% of the data to use as an illustration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf7nHKAfUwJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c68f597f-d4b7-4439-b346-3a46af711eb0"
      },
      "source": [
        "!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-29 14:26:02--  http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2144903 (2.0M) [application/x-gzip]\n",
            "Saving to: ‘kddcup.data_10_percent.gz’\n",
            "\n",
            "kddcup.data_10_perc 100%[===================>]   2.04M  3.27MB/s    in 0.6s    \n",
            "\n",
            "2020-04-29 14:26:03 (3.27 MB/s) - ‘kddcup.data_10_percent.gz’ saved [2144903/2144903]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvUoTeEqVG8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36aa7467-2672-43ff-893c-733a8eb90390"
      },
      "source": [
        "!hadoop fs -put kddcup* /"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: hadoop: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNF-8RvoVQ_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f38eee36-52e0-4ad3-a58b-bf27dcb82cd1"
      },
      "source": [
        "!hadoop fs -ls /"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: hadoop: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBjymfa-VZ5k",
        "colab_type": "text"
      },
      "source": [
        "#Reading in data\n",
        "The data are CSV files. In Spark, these can be read using textFile and splitting rows on commas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3uw_IVFVWgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession,SQLContext,Row\n",
        "\n",
        "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "datafile= \"hdfs:///kddcup.data_10_percent.gz\"\n",
        "raw_rdd = sc.textFile(data_file).cache()\n",
        "raw_rdd.take(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mat1b1giWHop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_rdd=raw_rdd.map(lambda row: row.split(\",\"))\n",
        "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
        "    duration=int(r[0]), \n",
        "    protocol_type=r[1],\n",
        "    service=r[2],\n",
        "    flag=r[3],\n",
        "    src_bytes=int(r[4]),\n",
        "    dst_bytes=int(r[5]),\n",
        "    wrong_fragment=int(r[7]),\n",
        "    urgent=int(r[8]),\n",
        "    hot=int(r[9]),\n",
        "    num_failed_logins=int(r[10]),\n",
        "    num_compromised=int(r[12]),\n",
        "    su_attempted=r[14],\n",
        "    num_root=int(r[15]),\n",
        "    num_file_creations=int(r[16]),\n",
        "    label=r[-1]\n",
        "    ))\n",
        "parsed_rdd.take(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9kq3aR1Wkvb",
        "colab_type": "text"
      },
      "source": [
        "# spark analysis\n",
        "\n",
        "One way to analyze data in Spark is to call methods on a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7htzt_0WpR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.createDataFrame(parsed_rdd)\n",
        "connections_by_protocol=df.groupby(\"protocol_type\").count().orderBy(\"count\",ascending=False)\n",
        "connections_by_protocol.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzqhn2O9XWO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.registerTempTable(\"connections\")\n",
        "attack_stats = sqlContext.sql(\"\"\"\n",
        "                           SELECT \n",
        "                             protocol_type, \n",
        "                             CASE label\n",
        "                               WHEN 'normal.' THEN 'no attack'\n",
        "                               ELSE 'attack'\n",
        "                             END AS state,\n",
        "                             COUNT(*) as total_freq,\n",
        "                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
        "                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
        "                             ROUND(AVG(duration), 2) as mean_duration,\n",
        "                             SUM(num_failed_logins) as total_failed_logins,\n",
        "                             SUM(num_compromised) as total_compromised,\n",
        "                             SUM(num_file_creations) as total_file_creations,\n",
        "                             SUM(su_attempted) as total_root_attempts,\n",
        "                             SUM(num_root) as total_root_acceses\n",
        "                           FROM connections\n",
        "                           GROUP BY protocol_type, state\n",
        "                           ORDER BY 3 DESC\n",
        "                           \"\"\")\n",
        "attack_stats.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uiOIy3eXrfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}