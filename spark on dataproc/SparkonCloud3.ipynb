{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkonCloud3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh-s7NTOZN0j",
        "colab_type": "text"
      },
      "source": [
        "# Migrating from Spark to BigQuery via Dataproc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98PFCvVSZVVh",
        "colab_type": "text"
      },
      "source": [
        "# Catch up: data to GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7elMKzEY_S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Catch up cell. Run if you did not do previous notebooks of this sequence\n",
        "!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
        "BUCKET=''  # CHANGE\n",
        "!pip install google-compute-engine\n",
        "!gsutil cp kdd* gs://$BUCKET/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwh31nwKZa2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET=''  # CHANGE\n",
        "!gsutil ls gs://$BUCKET/kdd*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdX54oPZhAy",
        "colab_type": "text"
      },
      "source": [
        "# Create a Python file\n",
        "Put all the code in a Python file. We can comment out the display-only code such as take() and show() Make changeable settings like BUCKET come from sys.args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmeoDjYoZmKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile spark_analysis.py\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "BUCKET = args.bucket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDw_y-J6Zyix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "from pyspark.sql import SparkSession, SQLContext, Row\n",
        "\n",
        "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "data_file = \"gs://{}/kddcup.data_10_percent.gz\".format(BUCKET)\n",
        "raw_rdd = sc.textFile(data_file).cache()\n",
        "#raw_rdd.take(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OSUzinwZ1Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
        "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
        "    duration=int(r[0]), \n",
        "    protocol_type=r[1],\n",
        "    service=r[2],\n",
        "    flag=r[3],\n",
        "    src_bytes=int(r[4]),\n",
        "    dst_bytes=int(r[5]),\n",
        "    wrong_fragment=int(r[7]),\n",
        "    urgent=int(r[8]),\n",
        "    hot=int(r[9]),\n",
        "    num_failed_logins=int(r[10]),\n",
        "    num_compromised=int(r[12]),\n",
        "    su_attempted=r[14],\n",
        "    num_root=int(r[15]),\n",
        "    num_file_creations=int(r[16]),\n",
        "    label=r[-1]\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tt8jafMZ5Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "df = sqlContext.createDataFrame(parsed_rdd)\n",
        "connections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\n",
        "connections_by_protocol.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij8HjmzdZ7yI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "df.registerTempTable(\"connections\")\n",
        "attack_stats = sqlContext.sql(\"\"\"\n",
        "                           SELECT \n",
        "                             protocol_type, \n",
        "                             CASE label\n",
        "                               WHEN 'normal.' THEN 'no attack'\n",
        "                               ELSE 'attack'\n",
        "                             END AS state,\n",
        "                             COUNT(*) as total_freq,\n",
        "                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
        "                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
        "                             ROUND(AVG(duration), 2) as mean_duration,\n",
        "                             SUM(num_failed_logins) as total_failed_logins,\n",
        "                             SUM(num_compromised) as total_compromised,\n",
        "                             SUM(num_file_creations) as total_file_creations,\n",
        "                             SUM(su_attempted) as total_root_attempts,\n",
        "                             SUM(num_root) as total_root_acceses\n",
        "                           FROM connections\n",
        "                           GROUP BY protocol_type, state\n",
        "                           ORDER BY 3 DESC\n",
        "                           \"\"\")\n",
        "attack_stats.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr__GUouZ-2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HdTQxcsaBRx",
        "colab_type": "text"
      },
      "source": [
        "# Write out report\n",
        "Make sure to copy the output to GCS so that we can safely delete the cluster. This has to be pure Python, so replace shell commands by equivalent Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO3KAPmDaCvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "ax[0].get_figure().savefig('report.png');\n",
        "#!gsutil rm -rf gs://$BUCKET/sparktobq/\n",
        "#!gsutil cp report.png gs://$BUCKET/sparktobq/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUpWCqW1aSvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "import google.cloud.storage as gcs\n",
        "bucket = gcs.Client().get_bucket(BUCKET)\n",
        "for blob in bucket.list_blobs(prefix='sparktobq/'):\n",
        "    blob.delete()\n",
        "bucket.blob('sparktobq/report.png').upload_from_filename('report.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUpmJC2aUmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile -a spark_analysis.py\n",
        "\n",
        "connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n",
        "    \"gs://{}/sparktobq/connections_by_protocol\".format(BUCKET))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kE4ZQ5RaWXI",
        "colab_type": "text"
      },
      "source": [
        "# Test automation\n",
        "Run it standalone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJbarye8aZhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET=''  # CHANGE\n",
        "print('Writing to {}'.format(BUCKET))\n",
        "!python spark_analysis.py --bucket=$BUCKET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOYGfZUDae0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil ls gs://$BUCKET/sparktobq/**"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}